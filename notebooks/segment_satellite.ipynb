{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6513,
     "status": "ok",
     "timestamp": 1575867937449,
     "user": {
      "displayName": "cvfinal project",
      "photoUrl": "",
      "userId": "01606994284983043688"
     },
     "user_tz": 300
    },
    "id": "cx9Vy4t1N-AX",
    "outputId": "9ebca1d1-9108-4f14-e59b-810cd272344c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
      "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-yt0yot9k\n",
      "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-yt0yot9k\n",
      "Requirement already satisfied (use --upgrade to upgrade): segmentation-models-pytorch==0.1.0 from git+https://github.com/qubvel/segmentation_models.pytorch in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.1.0) (0.4.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.1.0) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from segmentation-models-pytorch==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (4.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.17.4)\n",
      "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.0) (4.28.1)\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->segmentation-models-pytorch==0.1.0) (0.46)\n",
      "Building wheels for collected packages: segmentation-models-pytorch\n",
      "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.0-cp36-none-any.whl size=42616 sha256=de5f5bc7e841fedd566332fb55da659b0ab7bb12a7ebc3784d94d8dd9f59ce08\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mlzlrzpd/wheels/79/3f/09/1587a252e0314d26ad242d6d2e165622ab95c95e5cfe4b942c\n",
      "Successfully built segmentation-models-pytorch\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import random\n",
    "\n",
    "#!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unum0aTdOF-E"
   },
   "outputs": [],
   "source": [
    "class EventDataset(Dataset):\n",
    "  def __init__(self, pre_dir, post_dir, transform = torchvision.transforms.Compose(\n",
    "                  [torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "                   torchvision.transforms.ToTensor()])):\n",
    "    self.pre_dir = pre_dir\n",
    "    self.post_dir = post_dir\n",
    "    self.files_list = [s for s in os.listdir(pre_dir)]\n",
    "    self.transform = transform\n",
    "\n",
    "  def change_contrast(self, img, level):\n",
    "    factor = (259 * (level + 255)) / (255 * (259 - level))\n",
    "    def contrast(c):\n",
    "        return 128 + factor * (c - 128)\n",
    "    return img.point(contrast)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.files_list)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pre_name = os.path.join(self.pre_dir, self.files_list[idx])\n",
    "    #pre = self.change_contrast(Image.open(pre_name), 25)\n",
    "    pre = Image.open(pre_name)\n",
    "    pre = self.transform(pre)\n",
    "\n",
    "    post_name = os.path.join(self.post_dir, self.files_list[idx])\n",
    "    #post = self.change_contrast(Image.open(post_name), 25)\n",
    "    post = Image.open(post_name)\n",
    "    post = self.transform(post)\n",
    "    \n",
    "    return pre, post, self.files_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvNbYplIOIzA"
   },
   "outputs": [],
   "source": [
    "#pre_dir = '/content/drive/My Drive/cs461-final-project/data/disaster_images/pre_event'\n",
    "#post_dir = '/content/drive/My Drive/cs461-final-project/data/disaster_images/post_event'\n",
    "\n",
    "pre_dir = '../data/pre_event'\n",
    "post_dir = '../data/post_event'\n",
    "\n",
    "dataset = EventDataset(pre_dir, post_dir)\n",
    "data_loader = DataLoader(dataset, batch_size = 1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1575867946769,
     "user": {
      "displayName": "cvfinal project",
      "photoUrl": "",
      "userId": "01606994284983043688"
     },
     "user_tz": 300
    },
    "id": "cO3FKFbPTgjM",
    "outputId": "640b14b3-90a4-4fd0-f5aa-f39b595198b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_path = './best_model_full.pth'\n",
    "model_path = './best_model_unet.pth'\n",
    "\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79138,
     "status": "ok",
     "timestamp": 1575868751234,
     "user": {
      "displayName": "cvfinal project",
      "photoUrl": "",
      "userId": "01606994284983043688"
     },
     "user_tz": 300
    },
    "id": "5j0KOYG5OMir",
    "outputId": "b5804a25-0706-40e5-9c36-9004765dc15a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72a130422add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# helper function for data visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"PLot images in one row.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "#for i in range(10):\n",
    "i = 0\n",
    "while i in range(10):\n",
    "\n",
    "  n = np.random.choice(len(dataset))\n",
    "  pre, post, name = dataset[n]\n",
    "  pre = np.array(pre, dtype = np.float32)\n",
    "  post = np.array(post, dtype = np.float32)\n",
    "\n",
    "  pre_tensor = torch.from_numpy(pre).to(device).unsqueeze(0)\n",
    "  post_tensor = torch.from_numpy(post).to(device).unsqueeze(0)\n",
    "  pre_mask = model.predict(pre_tensor)\n",
    "  post_mask = model.predict(post_tensor)\n",
    "  pre_mask = pre_mask.squeeze().cpu().numpy().round()\n",
    "  post_mask = post_mask.squeeze().cpu().numpy().round()\n",
    "\n",
    "  pre_sum = np.sum(pre_mask)\n",
    "  post_sum = np.sum(post_mask)\n",
    "  threshold = 800\n",
    "  if pre_sum < threshold or post_sum < threshold:  \n",
    "    continue\n",
    "  print(pre_sum, post_sum)\n",
    "  print(name[:-4])\n",
    "\n",
    "  pre = np.transpose(pre, (1, 2, 0))\n",
    "  post = np.transpose(post, (1, 2, 0))\n",
    "  #np.set_printoptions(threshold=sys.maxsize)\n",
    "  #print(np.max(pre))\n",
    "\n",
    "  visualize(\n",
    "        pre_event_image= pre,\n",
    "        pre_event_mask = pre_mask, \n",
    "        post_event_image = post, \n",
    "        post_event_mask= post_mask\n",
    "    )\n",
    "  dest_dir = '../predictions_unet/'\n",
    "  #dest_dir = '../predictions_fpn/'\n",
    "  print(dest_dir + name[:-4] + '_' + 'pre' + '.jpg')\n",
    "  cv2.imwrite(dest_dir + name[:-4] + '_' + 'pre' + '.jpg', pre_mask*255)\n",
    "  cv2.imwrite(dest_dir + name[:-4] + '_' + 'post' + '.jpg', post_mask*255)\n",
    "  i = i +1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segment_satellite.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
